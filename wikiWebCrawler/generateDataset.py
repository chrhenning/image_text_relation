#!/usr/bin/env python3
# This tool creates a dataset from simplewiki articles. Therefore, it uses the output of two different wiki parsers [1] and [2].
# [1]: https://github.com/attardi/wikiextractor
# [2]: https://github.com/goldsmith/Wikipedia/tree/96854a468c68ad5644dd6f7564e200012517405a

# The output of [2] must be already preprocessed by the tool dumpWikiExtraction.py

import os
import sys
import pickle
import wikipedia as wiki
import argparse
import lxml.html
import hashlib
import urllib
import re
import json
import warnings
import traceback
import select

import preprocessImages as ppi

# output directory
#output = '.'

# articles are stored in JSON lines text format (http://jsonlines.org/)
FN_ARTICLES = 'articles.jsonl'
FN_ARTICLE_IDS = 'article_ids.txt'

# store a file and its metadata from wikimedia given the filename
def storeImageAndItsMeta(filename):
	global output
	
	assert(filename.startswith("/wiki/File:"))
	filename = filename[11:] # remove "/wiki/File:"
	metaFilename = os.path.splitext(filename)[0] + '.json'
	# each image in wikimedia is stored in a folder which has as name the two characters of the filename its md5 hash
	# this folder itself is within a folder which has as name the first character of the filename its md5 hash
	hashedName = hashlib.md5(urllib.parse.unquote(filename).encode()).hexdigest()
	folder1 = hashedName[0]
	folder2 = hashedName[:2]
	
	# we use the same folder structure in our output folder
	outputdir = os.path.join(output, folder1, folder2)
	if not os.path.exists(outputdir):
		os.makedirs(outputdir)
	
	urllib.request.urlretrieve("https://upload.wikimedia.org/wikipedia/commons/" + \
		folder1+"/"+folder2+"/"+filename, os.path.join(outputdir,filename))
	
	urllib.request.urlretrieve("https://simple.wikipedia.org/w/api.php?action=query&" + \
		"prop=imageinfo&iiprop=extmetadata&titles=File%3A"+filename+"&format=json", os.path.join(outputdir,metaFilename))
	
	return [os.path.join(folder1, folder2, filename), os.path.join(folder1, folder2, metaFilename)]

# extract links to other wiki articles
def extractKeyPhrasesFromHtml(htmlElems):
	links = []
	for elem in htmlElems:
		links += elem.findall('.//a')
		
	return [ a.attrib['title'] for a in links if \
		'title' in a.attrib.keys() and \
		a.attrib['href'].startswith('/wiki/') and \
		not a.attrib['href'].startswith('/wiki/File:')]

if __name__ == '__main__':
	global output

	parser = argparse.ArgumentParser()
	parser.add_argument('--input', type=str, help="The pickle dump generated by the script dumpWikiExtraction.py", \
		default='wikiextraction_dump.pickle')
	parser.add_argument('--output', type=str, help="Directory, where the generated dataset should be saved at. Note," + \
		" that its content will be overriden if it already exists.", default='./simplewiki-dataset')
	args = parser.parse_args()
	
	dumpFile = args.input
	output = args.output
	
	if os.path.isdir(output):
		print('Directory \'' + output + '\' already exists')
		print('Program will append articles according to the already processed articles stated in ' + FN_ARTICLE_IDS)
		#shutil.rmtree(output)
		
	if not os.path.exists(output):
		os.makedirs(output)
		
	if not os.path.isfile(dumpFile):
		raise Exception('File \'' + dumpFile + '\' does not exist')
		
	# load dumped articles into memory
	dumpedArticles = pickle.load(open(dumpFile,'rb'))
		
	# note, that we are dealing only with simplewiki articles in this program
	wiki.set_lang('simple')
	
	idFileName = os.path.join(output, FN_ARTICLE_IDS)
	articleFileName = os.path.join(output, FN_ARTICLES)
	
	# read article ids of articles already processed
	alreadyProcessed = set()
	if os.path.isfile(idFileName):
		with open(idFileName) as f:
			for l in f:
				if len(l.strip()) == 0:
					continue
				alreadyProcessed.add(int(l.strip()))
	
	idFile = open(idFileName, 'a')
	articleFile = open(articleFileName, 'a')
	
	# only in current program run
	numProcessedArticles = 0
	
	try:
		for aid, dumpedArticle in dumpedArticles.items():
			if aid in alreadyProcessed:
				continue
		
			print('Parsing article with id: ' + str(aid))
		
			page = None
			try:
				warnings.filterwarnings("ignore")
				page = wiki.page(pageid=aid, redirect=False)
				warnings.filterwarnings("always")
			except Exception as e:
				warnings.filterwarnings("always")
				warnings.warn('Page id ' + str(aid) + ' caused the following exception \'' + e.__class__.__name__ + '\' and will be ignored')	
				continue
		
			if dumpedArticle['meta']['title'] != page.title:
			  warnings.warn('Page id ' + str(aid) + ': Titles do not match: ' + dumpedArticle['meta']['title'] + ', ' + page.title)
			  continue	
		
			# extract pictures together with their relative position in the document
			dom = lxml.html.fromstring(page.html())
	
			# the dictionary where all the information of an article in our new dataset is stored
			# this dictionary is later dumped as json string into the output
			article = {}
			article['id'] = aid
			article['title'] = dumpedArticle['meta']['title']
			article['url'] = dumpedArticle['meta']['url']
			article['summary'] = page.summary.strip() #.replace('\n','')
			article['images'] = [] # images next to the summary
			try:
				article['references'] = page.references # from docu: May include external links within page that arenâ€™t technically cited anywhere.
			except KeyError: # the wikipedia implementation sometimes throws a keyerror for no obvious reason
				links = dom.findall('.//a')
				article['references'] = list(set([x.attrib['href'] for x in links \
					if 'external' in list(x.classes) and not x.attrib['href'].startswith('//simple.wikipedia.org/')]))
					
			article['categories'] = page.categories
			try:
				article['keyphrases'] = page.links
			except KeyError: # the wikipedia implementation sometimes throws a keyerror for no obvious reason
				article['keyphrases'] = extractKeyPhrasesFromHtml([dom])
	
			# array of all sections in this document
			# each section contains a list of subsections
			sections = []
			article['sections'] = sections
		
			# store number of current heading tag, which defines section depth
			headingStack = []
		
			sectionStack = []
			listOfAllSections = [] # easier accessible at the end than tree structure
			currSection = None
		
			# all p, h and li elements of the current section (used to gather keyphrases/keywords
			htmlElemsOrganized = []
			currHtmlElems = []
	
			for htmlElem in dom.getchildren():
				# search for headings (a heading defines a new section resp subsection)
				if re.match('h\d', htmlElem.tag):
					headingNum = int(htmlElem.tag[1])
			
					if not len(headingStack) == 0:
						while len(headingStack) > 0 and headingStack[-1] >= headingNum:
							headingStack.pop()
							sectionStack.pop()
					
					headingStack.append(headingNum)
					#print(' ' * int(headingStack[-1]) + str(headingStack))
					#print(' ' * int(headingStack[-1]) + htmlElem.getchildren()[0].text_content())
				
					# create new section object
					currSection = {}
					currSection['title'] = htmlElem.getchildren()[0].text_content().strip()
					currSection['subsections'] = []
					currSection['images'] = []
					currSection['text'] = ''
					currSection['lists'] = []
					currSection['keyphrases'] = []
					listOfAllSections.append(currSection)
				
					# if new section is subsection
					if len(sectionStack) > 0: 
						sectionStack[-1]['subsections'].append(currSection)
					else:
						sections.append(currSection)
					sectionStack.append(currSection)
				
					# a new section with new keywords starts
					currHtmlElems = [htmlElem]
					htmlElemsOrganized.append(currHtmlElems)
			
				#	search for images
				elif htmlElem.tag == 'div' and ('class' in htmlElem.attrib and htmlElem.attrib['class'].startswith('thumb')):
					assert(len(htmlElem.getchildren()) == 1)
					if len(htmlElem.getchildren()) == 1:
						thumbTag = htmlElem.getchildren()[0]
						assert('thumbinner' in list(thumbTag.classes))
						if 'class' in thumbTag.attrib and thumbTag.attrib['class'].startswith('thumbinner'):
							possibleImgDivs = [] 
							
							# there is only one image in the div
							if len(thumbTag.getchildren()) == 2 and 'image' in thumbTag.getchildren()[0].classes:
								possibleImgDivs.append((thumbTag.getchildren()[0], thumbTag.getchildren()[1]))
					
							if len(possibleImgDivs) == 0: #there are possibly more images in this div
								childTags = thumbTag.getchildren()
								
								if set([t.tag for t in childTags]) == set(['div']): # childs are all div tags, otherwise it might be simply another multimedia element
									for childTag in childTags:
										if len(childTag.getchildren()) == 2 and 'image' in childTag.getchildren()[0].classes:
											possibleImgDivs.append((childTag.getchildren()[0], childTag.getchildren()[1]))
										elif len(childTag.getchildren()) == 2 and 'thumbimage' in childTag.getchildren()[0].classes:
											assert('image' in childTag.getchildren()[0].getchildren()[0].classes)
											possibleImgDivs.append((childTag.getchildren()[0].getchildren()[0], childTag.getchildren()[1]))
								
							# for all images in this thumb
							for imgLink, imgCaption in possibleImgDivs:							
							
								assert(imgCaption.attrib['class'] == 'thumbcaption')
						
								imgObj = {}
								imgObj['filename'] = imgLink.attrib['href']
								imgObj['caption'] = imgCaption.text_content()
								imgObj['keyphrases'] = extractKeyPhrasesFromHtml([imgCaption])
								if currSection is None:
									article['images'].append(imgObj)
									imgObj['imgpath'], imgObj['metapath'] = storeImageAndItsMeta(imgObj['filename']) # we always download the images belonging to the current article
								else:
									currSection['images'].append(imgObj)
							
				# gather all text from the current section
				elif htmlElem.tag == 'p' and currSection != None: 
					currHtmlElems.append(htmlElem)
			
					currSection['text'] += htmlElem.text_content()
				
				# gather all lists from the current section
				elif htmlElem.tag == 'ul' and currSection != None: 
					currHtmlElems.append(htmlElem)
			
					currList = []
					currSection['lists'].append(currList)
					for listElem in htmlElem.getchildren():
						currList.append(listElem.text_content())
					
			# delete reference sections if existing (we don't need it, it would be empty anyway and we already stored the references)
			for sec in reversed(sections):
				if sec['title'] == 'References':
					sections.remove(sec)
					break
		
			# download pictures from those sections, that have at least one text element (p-tag) and gather keyphrases of sections
			for sec in listOfAllSections:
				sec['text'] = sec['text'].strip() # we don't need leading and following newlines
		
				if len(sec['text'].strip()) > 0: # download all images in this section
					for imgObj in sec['images']:
						imgObj['imgpath'], imgObj['metapath'] = storeImageAndItsMeta(imgObj['filename'])
					
				secHtmlElems = htmlElemsOrganized.pop(0)
				sec['keyphrases'] = extractKeyPhrasesFromHtml(secHtmlElems)
			
			assert(len(htmlElemsOrganized) == 0)
				
			# preprocess all the images in this article
			ppi.preprocess_images(article['images'])
			ppi.iterate_sections(article['sections'])
							
			jsonLine = json.dumps(article)			
			articleFile.write(jsonLine + '\n')
			idFile.write(str(aid)+'\n')
		
			numProcessedArticles += 1
			if numProcessedArticles > 0 and numProcessedArticles % 20 == 0:
				print('### Already processed ' + str(numProcessedArticles) + ' articles')

				# check if user wants to close the program now
				timeout = 10
				print('Press any key to close program now. The program will resume on the current state on restart.')
				prompt = "You have %d seconds to close the program ..." % timeout
				print(prompt)
				
				i, o, e = select.select([sys.stdin], [], [], timeout)
				if (i):
					idFile.close()
					articleFile.close()
					print('Program stopped ...')
					sys.exit()
				else:
					print('No key pressed. Continuing work ...')

	except Exception as e:
		traceback.print_exc()

	idFile.close()
	articleFile.close()
	
	
	

#!/usr/bin/env python3
# This tool will preprocess the wiki dataset which is stored in a jsonl file (usually: articles.jsonl)
# Preprocessing here means simply to bring it in an appropriate form that allows easy access to image, text pairs
# and to split the corresponding text already into sentences
# Other preprocessing steps: References are filtered

import argparse
import os
import nltk
import json
import sys
import logging
import traceback
import re

sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')

logging.basicConfig(level=logging.DEBUG)

# this method will filter out wikipedia references from the text, since they are not useful to us
def filter_references(text):
  p = re.compile('\[[\w\s\?]*\]')
   
  for tup in text:
    for item in tup:
      for i, s in enumerate(item):
        # replace references with empy strings
        item[i] = p.sub('',s)

# process all given images (the given text must be the corresponding text for this image)
# the format of the given text is a list of tuples (title, text), whereas images, that belong to a single section,
# have only one tuple.
def preprocess_pair(article, images, text):
  global OUTPUT_FILE, PROCESSED_IMAGES
  
  filter_references(text)
  
  for img in images:
    # only if the images was preprocessed and converted into a jpeg image it will be converted
    if 'origformat' in img:
      assert len(text) > 0 and len(text[0][1]) > 0

      PROCESSED_IMAGES += 1
      pair= {}
      pair['id'] = article['id'] # article id
      pair['text'] = text
      pair['imgpath'] = img['imgpath']
      pair['origformat'] = img['origformat']
      pair['caption'] = img['caption']
      
      # write image text pair into output file
      json_line = json.dumps(pair)			
      OUTPUT_FILE.write(json_line + '\n')
  

# recursively iterate through sectins and subsection and preprocess all occuring images
def iterate_sections(article, sections):
  for sec in sections:
    preprocess_pair(article, sec['images'], [([sec['title']], sent_detector.tokenize(sec['text'].strip()))])
    iterate_sections(article, sec['subsections'])

def get_sections_text(sections):
  text = []
  for sec in sections:
    '''
    temp = []
    temp.append(([sec['title']], sent_detector.tokenize(sec['text'].strip())))
    temp += get_sections_text(sec['subsections'])
    
    # join titles until the first text has been found
    titles = []
    for i, tup in enumerate(temp):
      titles += tup[0]
      if len(tup[1]) == 0:
        continue
      if len(tup[1]) > 0:
        # delete all previous tuples with empty content
        for j in range(i):
          temp.pop(0)
        temp[0] = (titles, tup[1])
        titles = []
    # only add the extracted section, if there is at least one subsection containing actual content
    if len(titles) == 0:
      text += temp      
    '''
    
    sentences = sent_detector.tokenize(sec['text'].strip())
    if len(sentences) > 0:    
      text.append(([sec['title']], sentences))
    temp = get_sections_text(sec['subsections'])
    
    # concatenate current title in front of all subsections
    for tup in temp:
      tup[0].insert(0, sec['title']) 
      text.append(tup)

  return text

global PROCESSED_IMAGES
PROCESSED_IMAGES = 0

global OUTPUT_FILE

if __name__ == '__main__':

  parser = argparse.ArgumentParser()
  parser.add_argument('--data', type=str, help="Directory, that contains the dataset", default='./simplewiki-dataset')
  parser.add_argument('--outname', type=str, help="The name of the jsonl file, that is generated by this script.", default='wiki-samples.jsonl')
  args = parser.parse_args()

  inputdir = args.data
  outname = os.path.join(inputdir, args.outname)

  dataset_file = os.path.join(inputdir, 'articles.jsonl')

  if not os.path.isdir(inputdir):
    raise(Exception('Directory ' + inputdir + ' does not exist.'))

  if not os.path.isfile(dataset_file):
    raise(Exception('File ' + dataset_file + ' does not exist.'))
    
  OUTPUT_FILE = open(outname, 'w')
    
  try:


    iteration = 0
    # for each wiki article
    with open(dataset_file, 'r') as df:
      for line in df:
        article = json.loads(line)

        article_img_exists = False
        for img in article['images']:
          if 'origformat' in img:
            article_img_exists = True
            break
            
        if article_img_exists:
          # generate text of whole article
          text = [([article['title']], sent_detector.tokenize(article['summary'].strip()))]
          text += get_sections_text(article['sections'])
          preprocess_pair(article, article['images'], text)


        iterate_sections(article, article['sections'])

        #if iteration == 2:
        #  break

        iteration += 1
        if not iteration % 50:
          logging.info('Processed %d  articles with %d image-text-pairs' % (iteration, PROCESSED_IMAGES)) 
          		
  except Exception as e:
    traceback.print_exc()
    OUTPUT_FILE.close()
    sys.exit()
    
  logging.info('Summary: processed %d articles with %d image-text-pairs' % (iteration, PROCESSED_IMAGES)) 
  OUTPUT_FILE.close()

